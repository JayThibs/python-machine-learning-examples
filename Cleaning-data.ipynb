{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data\n",
    "\n",
    "Garbage in, garbage out! A big part of the data science process involves cleaning the data before using it. There are many reasons why this is important. Whether you clean your data or not can make the difference between having a good model vs a bad one. You should not take data cleaning lightly, even if it's more fun to work on your machine learning model. Here's a list of things you want to look for when you are cleaning data:\n",
    "\n",
    "* Outliers: the machine logging the data may have malfunctionned for a bit and recorded a value that makes no sense compared to the rest of the data. Or there could be someone using a bot that keeps coming to your site and you only want to take into account real users. You want to elimate outliers from your dataset.\n",
    "\n",
    "* Missing Data: It could be that data was simply not recorded at a particular time, but it could also mean another class for classification. You need to make the proper decision here.\n",
    "\n",
    "* Malicious Data: There might be people trying to mess with your website's recommender system by trying to promote their item (movie on Netflix, product on Amazon, etc).\n",
    "\n",
    "* Erroneous Data: the machine may be making some mistakes during the data collection or there might be a mistake when merging data together.\n",
    "\n",
    "* Irrelevant Data: your dataset contains data regarding things you don't want to model. As in, you have data on schools across Canada, but you only want data on schools in Toronto.\n",
    "\n",
    "* Inconsistent Data: for example, people could write addresses differently (not writing \"street\" or \"drive\", may add the city and country, etc). If you're looking at movies, may be the same movie has a different name in different countries.\n",
    "\n",
    "* Formatting: date formatting can be different depending on the country.\n",
    "\n",
    "We'll be taking a web access log, and figure out the most-viewed pages on the website.\n",
    "\n",
    "Let's start by setting up a regex that lets us parse an Apache access log line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "format_pat = re.compile(\n",
    "    r\"(?P<host>[\\d\\.]+)\\s\"\n",
    "    r\"(?P<identity>S*)\\s\"\n",
    "    r\"(?P<user>S*)\\s\"\n",
    "    r\"\\[(?P<time>.*?)\\]\\s\"\n",
    "    r'\"(?P<request>.*?)\"\\s'\n",
    "    r\"(?P<status>\\d+)\\s\"\n",
    "    r\"(?P<bytes>\\S*)\\s\"\n",
    "    r'\"(?P<referer>.*?)\"\\s'\n",
    "    r'\"(?P<user_agent>.*?)\"\\s*'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logPath = \"/Users/jacquesthibodeau/Python Data Science/access_log.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to write a script that extracts the URL in each access, and use a dictionnary to count up the number of times each one appears. Then we'll sort it and print out the top 20 pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
